{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f8ff59",
   "metadata": {},
   "source": [
    "# SPMpy I/O Library v0.1 (Notebook-paired)\n",
    "\n",
    "This notebook contains the **I/O function set** intended to live under `spmpy/io/`.\n",
    "It is designed to be **paired with a `.py` file via jupytext**.\n",
    "\n",
    "## Goals\n",
    "- Preserve the legacy workflow interface where it makes sense.\n",
    "- Avoid hidden global side effects (especially `os.chdir()`).\n",
    "- Keep I/O responsibilities limited to: **read + standardize to `xarray.Dataset`**.\n",
    "\n",
    "## Included functions\n",
    "- `select_folder()` — GUI folder picker (PyQt5)\n",
    "- `files_in_folder()` — inventory a folder into a `pandas.DataFrame` (**no `chdir`**)\n",
    "- `img2xr()` — load `.sxm` into `xarray.Dataset` (NetCDF-safe attrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da082186",
   "metadata": {},
   "source": [
    "## Why we avoid `os.chdir()`\n",
    "\n",
    "`os.chdir()` changes the **process-wide current working directory**. In a notebook workflow, this can silently\n",
    "affect unrelated cells and libraries that use relative paths.\n",
    "\n",
    "### The design used here\n",
    "- We keep your **working folder** as an explicit variable, e.g. `folder_path`.\n",
    "- We store **full paths** for each file in the inventory DataFrame (`file_path`).\n",
    "\n",
    "### Implication for saving results\n",
    "Yes, this means that **saving should also use explicit paths**. For example:\n",
    "\n",
    "- If you want outputs to go next to the raw data: use `output_dir = folder_path`.\n",
    "- If you want a clean separation: use `output_dir = Path(folder_path) / 'processed'`.\n",
    "\n",
    "In other words, you choose the target folder once (explicitly), then every save uses that folder.\n",
    "This is more reproducible than relying on whatever the current working directory happens to be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c403628",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "These are standard dependencies for the I/O layer.\n",
    "(`nanonispy` is required only when `img2xr()` is called.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440160db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Optional GUI dependency (only needed when select_folder() is used)\n",
    "try:\n",
    "    from PyQt5.QtWidgets import QApplication, QFileDialog\n",
    "except Exception:\n",
    "    QApplication = None\n",
    "    QFileDialog = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee283c",
   "metadata": {},
   "source": [
    "## `select_folder()`\n",
    "\n",
    "Folder picker used in Quickstart Stage-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e09a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_folder() -> str:\n",
    "    \"\"\"Open a folder selection dialog and return the selected folder path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Selected folder path. Empty string if no folder was selected.\n",
    "    \"\"\"\n",
    "    if QApplication is None or QFileDialog is None:\n",
    "        raise ModuleNotFoundError(\n",
    "            \"PyQt5 is required for select_folder(). Install PyQt5 or use a non-GUI path workflow.\"\n",
    "        )\n",
    "\n",
    "    app = QApplication.instance()\n",
    "    if app is None:\n",
    "        import sys\n",
    "        app = QApplication(sys.argv)\n",
    "\n",
    "    file_dialog = QFileDialog()\n",
    "    folder_path = file_dialog.getExistingDirectory(None, \"Select Folder\")\n",
    "    return str(folder_path) if folder_path else \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f9c16f",
   "metadata": {},
   "source": [
    "## `files_in_folder()` (no `chdir`)\n",
    "\n",
    "This function inventories a folder and returns a DataFrame with the **same columns** as your legacy workflow:\n",
    "\n",
    "- `group`, `num`, `file_name`, `type`\n",
    "\n",
    "Additionally, it includes two columns that make multi-folder workflows safer:\n",
    "\n",
    "- `folder_path` — the folder that was scanned\n",
    "- `file_path` — full path to each file\n",
    "\n",
    "Because `file_path` is explicit, later stages can load and save deterministically without relying on `os.chdir()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_in_folder(path_input: str, print_all: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Generate a DataFrame listing files in the specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_input : str\n",
    "        Folder path.\n",
    "    print_all : bool, optional\n",
    "        If True, prints the full DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with columns: ['group', 'num', 'file_name', 'type', 'folder_path', 'file_path'].\n",
    "    \"\"\"\n",
    "    folder = Path(path_input)\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Folder does not exist: {folder}\")\n",
    "\n",
    "    # Keep this informational print (legacy-friendly) WITHOUT changing CWD.\n",
    "    print(\"Current Path =\", os.getcwd())\n",
    "    print(\"Target Folder =\", str(folder))\n",
    "\n",
    "    # Inventory by extension (no chdir)\n",
    "    sxm_files  = sorted([p.name for p in folder.glob('*.sxm')])\n",
    "    grid_files = sorted([p.name for p in folder.glob('*.3ds')])\n",
    "    csv_files  = sorted([p.name for p in folder.glob('*.csv')])\n",
    "    gwy_files  = sorted([p.name for p in folder.glob('*.gwy')])\n",
    "    xlsx_files = sorted([p.name for p in folder.glob('*.xlsx')])\n",
    "    nc_files   = sorted([p.name for p in folder.glob('*.nc')])\n",
    "\n",
    "    def _df_for(files, ext_len, num_slice=True):\n",
    "        rows = []\n",
    "        for fn in files:\n",
    "            if num_slice:\n",
    "                group = fn[:-7]\n",
    "                num = fn[-7:-4]\n",
    "            else:\n",
    "                group = fn[:-ext_len]\n",
    "                num = np.nan\n",
    "            rows.append([group, num, fn])\n",
    "        return pd.DataFrame(rows, columns=['group', 'num', 'file_name'])\n",
    "\n",
    "    file_list_sxm_df  = _df_for(sxm_files,  4, num_slice=True)\n",
    "    file_list_3ds_df  = _df_for(grid_files, 4, num_slice=True)\n",
    "    file_list_csv_df  = _df_for(csv_files,  4, num_slice=True)\n",
    "    file_list_gwy_df  = _df_for(gwy_files,  4, num_slice=False)\n",
    "    file_list_xlsx_df = _df_for(xlsx_files, 5, num_slice=False)\n",
    "    file_list_nc_df   = _df_for(nc_files,   3, num_slice=False)\n",
    "\n",
    "    file_list_df = pd.concat(\n",
    "        [file_list_sxm_df, file_list_3ds_df, file_list_csv_df,\n",
    "         file_list_gwy_df, file_list_xlsx_df, file_list_nc_df],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    file_list_df['type'] = [fn[-3:] for fn in file_list_df.file_name]\n",
    "    file_list_df.loc[file_list_df.type == 'lsx', 'type'] = 'xlsx'\n",
    "    file_list_df.loc[file_list_df.type == '.nc', 'type'] = 'nc'\n",
    "\n",
    "    # Add explicit paths\n",
    "    file_list_df['folder_path'] = str(folder)\n",
    "    file_list_df['file_path'] = [str(folder / fn) for fn in file_list_df.file_name]\n",
    "\n",
    "    if print_all:\n",
    "        print(file_list_df)\n",
    "\n",
    "    # Legacy-style summary prints\n",
    "    sxm_file_groups = list(set(file_list_sxm_df['group']))\n",
    "    for group in sxm_file_groups:\n",
    "        print('sxm file groups:', group, ': # of files =',\n",
    "              len(file_list_sxm_df[file_list_sxm_df['group'] == group]))\n",
    "\n",
    "    if len(file_list_df[file_list_df['type'] == '3ds']) == 0:\n",
    "        print('No GridSpectroscopy data')\n",
    "    else:\n",
    "        print('# of GridSpectroscopy',\n",
    "              list(set(file_list_df[file_list_df['type'] == '3ds'].group))[0],\n",
    "              '=', file_list_df[file_list_df['type'] == '3ds'].group.count())\n",
    "\n",
    "    return file_list_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7731d06",
   "metadata": {},
   "source": [
    "## `grid2xr()` (3DS → `xarray.Dataset`)\n",
    "\n",
    "This function loads Nanonis GridSpectroscopy `.3ds` files and standardizes them into an `xarray.Dataset`.\n",
    "\n",
    "Design rules:\n",
    "- I/O only: reading + metadata/coords standardization.\n",
    "- No plane fit / flattening / filtering here.\n",
    "\n",
    "If `nanonispy` (or other required dependencies) are missing, the function should raise a clear error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#griddata_file = file_list_df[file_list_df.type=='3ds'].iloc[0].file_name\n",
    "\n",
    "def grid2xr(griddata_file, center_offset = True): \n",
    "    \"\"\"\n",
    "    An xarray DataSet representing grid data from a Nanonis 3ds file.\n",
    "\n",
    "    This DataSet contains multiple variables corresponding to different data channels, such as \"I_fwd\" (Forward Current), \"I_bwd\" (Backward Current), \"LIX_fwd\" (Lock-In X Forward), \"LIX_bwd\" (Lock-In X Backward), and \"topography\" (Topography). The data is organized along three dimensions: \"Y\" (Y-coordinate), \"X\" (X-coordinate), and \"bias_mV\" (Bias Voltage in mV).\n",
    "\n",
    "    Attributes:\n",
    "        - title (str): A title or description of the grid data.\n",
    "        - image_size (list): A list containing the size of the image in X and Y dimensions.\n",
    "        - X_spacing (float): The spacing between X-coordinates in nanometers.\n",
    "        - Y_spacing (float): The spacing between Y-coordinates in nanometers.\n",
    "\n",
    "    Additional Information:\n",
    "    - The \"bias_mV\" dimension represents the bias voltage values in mV, and it includes values that are adjusted to have a \"zero\" bias point.\n",
    "    - Depending on the `center_offset` parameter used during conversion, the X and Y coordinates may be adjusted to represent positions in the real scanner field of view or with (0,0) as the origin of the image.\n",
    "\n",
    "    Example Usage:\n",
    "\n",
    "    Convert a Nanonis 3ds file to a grid_xr DataSet\n",
    "    grid_xr = grid2xr(\"example.3ds\")\n",
    "\n",
    "    Access data variables\n",
    "    topography_data = grid_xr[\"topography\"]\n",
    "    forward_current_data = grid_xr[\"I_fwd\"]\n",
    "\n",
    "    Access attributes\n",
    "    title = grid_xr.attrs[\"title\"]\n",
    "    image_size = grid_xr.attrs[\"image_size\"]\n",
    "    x_spacing = grid_xr.attrs[\"X_spacing\"]\n",
    "    y_spacing = grid_xr.attrs[\"Y_spacing\"]\n",
    "\n",
    "\n",
    "    Note: This DataSet is suitable for further analysis, visualization, and manipulation using the xarray library in Python.\n",
    "\n",
    "\n",
    "    ---\n",
    "    Summary \n",
    "    \n",
    "    Here's a breakdown of the main steps in the grid2xr function:\n",
    "    Read the Nanonis 3ds file using NanonisFile and extract relevant information such as grid dimensions, position, size, step sizes, channels (e.g., topography, current), and bias values.\n",
    "    Check the topography data and reshape it if necessary. This step is for handling cases where the topography data is not in the expected shape.\n",
    "    Process and interpolate bias values to ensure they include \"zero\" bias and have an odd number of points. This step is necessary to account for different bias settings in the data.\n",
    "    Interpolate the current and lock-in data (both forward and backward) to match the new bias values.\n",
    "    Create an xarray DataSet named grid_xr with the following variables: \"I_fwd,\" \"I_bwd,\" \"LIX_fwd,\" \"LIX_bwd,\" and \"topography.\" These variables are associated with dimensions \"Y,\" \"X,\" and \"bias_mV.\"\n",
    "    Assign various attributes to the grid_xr DataSet, including the title, image size, spacing, and frequency information.\n",
    "    Optionally, adjust the scan center position in real scanner field-of-view based on the center_offset parameter.\n",
    "    Check and handle cases where the XY dimensions are not equal and may require interpolation.\n",
    "    Return the grid_xr DataSet as the result of the function.\n",
    "    This function seems to be designed for specific data formats and processing tasks related to Nanonis data. You can call this function with a Nanonis 3ds file as input to convert it into an xarray DataSet with the described attributes and dimensions.\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Import necessary libraries\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "    import scipy.interpolate as sp\n",
    "    import nanonispy as nap\n",
    "\n",
    "    file = griddata_file\n",
    "    #####################\n",
    "    # conver the given 3ds file\n",
    "    # to  xarray DataSet (check the attributes)\n",
    "    NF = nap.read.NanonisFile(file)\n",
    "    Gr = nap.read.Grid(NF.fname)#\n",
    "    channel_name = Gr.signals.keys()  \n",
    "    #print (channel_name)\n",
    "    N = len(file);\n",
    "    f_name = file[0:N-4]\n",
    "    print (f_name) # Gr.basename\n",
    "    # Extract data from the Nanonis file\n",
    "    #####################################\n",
    "    #  Header part\n",
    "    #  Gr.header\n",
    "    #####################################\n",
    "    [dim_px,dim_py] = Gr.header['dim_px'] \n",
    "    [cntr_x, cntr_y] = Gr.header['pos_xy']\n",
    "    [size_x,size_y] = Gr.header['size_xy']\n",
    "    [step_dx,step_dy] = [ size_x/dim_px, size_y/dim_py] \n",
    "    #pixel_size =  size / pixel \n",
    "    '''\n",
    "    ####   nX, nY --> x,y real scale  np array \n",
    "    #nX = np.array([step_dx*(i+1/2) for i in range (0,dim_px)])\n",
    "    #nY = np.array([step_dy*(i+1/2) for i in range (0,dim_py)])\n",
    "\n",
    "    # x = cntr_x - size_x + nX\n",
    "    # y = cntr_y - size_y + nY\n",
    "    # real XY position in nm scale, Center position & scan_szie + XY position\n",
    "    '''\n",
    "\n",
    "    ### Correct X and Y values extraction\n",
    "    x = np.linspace(cntr_x - size_x / 2, cntr_x + size_x / 2, dim_px)\n",
    "    y = np.linspace(cntr_y - size_y / 2, cntr_y + size_y / 2, dim_py)\n",
    "\n",
    "    \n",
    "    #####################################\n",
    "    # signal part\n",
    "    # Gr.signals\n",
    "    #####################################\n",
    "    topography = Gr.signals['topo']\n",
    "    params_v = Gr.signals['params'] \n",
    "    # params_v.shape = (dim_px,dim_py,15) \n",
    "    # 15: 3ds infos. \n",
    "    bias = Gr.signals['sweep_signal']\n",
    "    # check the shape (# of 'original' bias points)\n",
    "\n",
    "    ##########################################\n",
    "    # * if there is no bwd --> (bwd <= fwd)\n",
    "    # * fwd bwd data average \n",
    "    ##########################################\n",
    "\n",
    "    \n",
    "    I_fwd = Gr.signals['Current (A)'] # 3d set (dim_px,dim_py,bias)\n",
    "    #I_bwd = Gr.signals['Current [bwd] (A)'] # I bwd\n",
    "    try:\n",
    "        I_bwd = Gr.signals['Current [bwd] (A)']\n",
    "    except KeyError: # if bwd channel was not saved.\n",
    "        I_bwd = I_fwd\n",
    "        print (\"there is no [bwd] channel\")\n",
    "    # sometimes, LI channel names are inconsistent depends on program ver. \n",
    "    # find 'LI Demod 1 X (A)'  or  'LI X 1 omega (A)'\n",
    "\n",
    "    #print( [s  for s in Gr.signals.keys()  if \"LI\"  in s  if \"X\" in s ])\n",
    "    # 'LI' & 'X' in  channel name (signal.keys) \n",
    "    LIX_keys = [s  for s in Gr.signals.keys()  if \"LI\"  in s  if \"X\" in s ]\n",
    "    # 0 is fwd, 1 is bwd \n",
    "    '''LIX_fwd, LIX_bwd = Gr.signals[LIX_keys[0]] ,Gr.signals[LIX_keys[1] ]'''\n",
    "    if len(LIX_keys) == 2:\n",
    "        LIX_fwd, LIX_bwd = Gr.signals[LIX_keys[0]], Gr.signals[LIX_keys[1]]\n",
    "    elif len(LIX_keys) == 1:\n",
    "        # If LIX_keys list length is 1\n",
    "        LIX_fwd = Gr.signals[LIX_keys[0]]\n",
    "        LIX_bwd = Gr.signals[LIX_keys[0]]\n",
    "    else: \n",
    "        print (\"Define LIX again\")\n",
    "    # same for LIY\n",
    "    #print( [s  for s in Gr.signals.keys()  if \"LI\"  in s  if \"Y\" in s ])\n",
    "    # 'LI' & 'Y' in  channel name (signal.keys) \n",
    "    LIY_keys = [s  for s in Gr.signals.keys()  if \"LI\"  in s  if \"Y\" in s ]\n",
    "    # 0 is fwd, 1 is bwd \n",
    "    '''LIY_fwd, LIY_bwd = Gr.signals[LIY_keys[0]] ,Gr.signals[LIY_keys[1] ]'''\n",
    "    if len(LIY_keys) == 2:\n",
    "        LIY_fwd, LIY_bwd = Gr.signals[LIY_keys[0]], Gr.signals[LIY_keys[1]]\n",
    "    elif len(LIY_keys) == 1:\n",
    "        # If LIX_keys list length is 1\n",
    "        LIY_fwd = Gr.signals[LIY_keys[0]]\n",
    "        LIY_bwd = Gr.signals[LIY_keys[0]]\n",
    "    else: \n",
    "        print (\"Define LIY again\")\n",
    "    ###########################################################\n",
    "    #plt.imshow(topography) # toppography check\n",
    "    #plt.imshow(I_fwd[:,:,0]) # LIX  check\n",
    "    ###########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    #\t\t Title for Grid data \n",
    "    #       grid size, pixel, bias condition, and so on.\n",
    "    #############################################################\n",
    "    # Gr.header.get('Bias>Bias (V)') # bias condition \n",
    "    # Gr.header.get('Z-Controller>Setpoint') # current set  condition\n",
    "    # Gr.header.get('dim_px')  # jpixel dimension \n",
    "    title = Gr.basename +' \\n ('  + str(\n",
    "        float(Gr.header.get('Bias Spectroscopy>Sweep Start (V)'))\n",
    "    ) +' V ~ ' +str( \n",
    "        float(Gr.header.get('Bias Spectroscopy>Sweep End (V)'))\n",
    "    )+ ' V) \\n at Bias = '+ Gr.header.get(\n",
    "        'Bias>Bias (V)'\n",
    "    )[0:-3]+' mV, I_t =  ' + Gr.header.get(\n",
    "        'Z-Controller>Setpoint'\n",
    "    )[0:-4]+ ' pA, '+str(\n",
    "        Gr.header.get('dim_px')[0]\n",
    "    )+' x '+str(\n",
    "        Gr.header.get('dim_px')[1]\n",
    "    )+' points'\n",
    "    #############################################################       \n",
    "    ###########################\n",
    "    # Bias segment check      #\n",
    "    ###########################\n",
    "    if 'Segment Start (V), Segment End (V), Settling (s), Integration (s), Steps (xn), Lockin, Init. Settling (s)' in Gr.header.keys():\n",
    "            segment_info = Gr.header['Segment Start (V), Segment End (V), Settling (s), Integration (s), Steps (xn), Lockin, Init. Settling (s)']\n",
    "            segments = [list(map(float, seg.split(',')[:5])) for seg in segment_info]\n",
    "            print ('bias sweep is using Segment')\n",
    "            \n",
    "            bias_segment = []\n",
    "            for start, end, _, _, steps in segments:\n",
    "                bias_segment.extend(np.linspace(start, end, int(steps)))\n",
    "            \n",
    "            #print ('bias_segment: ', bias_segment)\n",
    "            # 중복 제거 (세그먼트 연결 부분)\n",
    "            def remove_duplicates(arr):\n",
    "                seen = set()\n",
    "                result = []\n",
    "                for item in arr:\n",
    "                    if item not in seen:\n",
    "                        result.append(item)\n",
    "                        seen.add(item)\n",
    "                return result\n",
    "        \n",
    "            bias_segment = remove_duplicates(bias_segment)\n",
    "            # use the 'remove_duplicates' function instead of np.unique \n",
    "            # in order not to mix the bias segment ascending /descending order\n",
    "            # print ('after remove duplicate bias_segment: ', bias_segment)\n",
    "            if len(bias_segment) != I_fwd.shape[2]:\n",
    "                print(\"Error: check Segment\")\n",
    "                return None\n",
    "            \n",
    "            # 가장 작은 간격을 절대값 기준으로 계산\n",
    "            min_step = np.min(np.abs(np.diff(bias_segment)))\n",
    "            print('min_step:', min_step)\n",
    "            \n",
    "            # bias_segment의 시작과 끝을 비교하여 bias_new 생성\n",
    "            if bias_segment[0] > bias_segment[-1]:\n",
    "                # 시작점이 양수인 경우 (양수에서 음수로 가는 경우)\n",
    "                bias_new = np.arange(bias_segment[0], bias_segment[-1] - min_step, -min_step)\n",
    "            else:\n",
    "                # 시작점이 음수인 경우 (음수에서 양수로 가는 경우)\n",
    "                bias_new = np.arange(bias_segment[0], bias_segment[-1] + min_step, min_step)\n",
    "            \n",
    "            #print('bias_new:', bias_new)\n",
    "            \n",
    "            # bias_new의 방향을 설정\n",
    "            if bias_new[0] > bias_new[-1]:\n",
    "                bias_segment_direction_change = False\n",
    "                print('bias_segment_direction_change = False')\n",
    "            else:\n",
    "                bias_segment_direction_change = True\n",
    "                print('bias_segment_direction_change = True')\n",
    "                print('Flipped bias_new to start from positive and end at negative')\n",
    "            '''\n",
    "            # 가장 작은 간격을 기준으로 bias_new 생성\n",
    "            min_step = np.min( (np.diff(bias_segment)))\n",
    "            print ('min_step',min_step)\n",
    "            bias_new = np.arange(bias_segment[0], bias_segment[-1] + min_step, min_step)\n",
    "            print ('bias_new: ', bias_new)\n",
    "            # bias_new의 방향을 양수에서 음수로 설정\n",
    "            if bias_new[0] > bias_new[-1]:\n",
    "                # bias_new가 양수에서 음수 방향이면 그대로 유지\n",
    "                bias_segment_direction_change = False\n",
    "                print ('bias_segment_direction_change = False')\n",
    "                pass\n",
    "            else:\n",
    "                # bias_new가 음수에서 양수 방향이면 반전시킴\n",
    "                bias_new = np.flip(bias_new)\n",
    "                bias_segment_direction_change = True\n",
    "                print ('bias_segment_direction_change = True')\n",
    "                print('Flipped bias_new to start from positive and end at negative')\n",
    "            '''\n",
    "            # bias_new가 0을 포함하도록 조정\n",
    "            if len(bias_new) % 2 == 0:\n",
    "                bias_new = np.linspace(bias_new[0], bias_new[-1], len(bias_new) + 1)\n",
    "            \n",
    "            nearest_zero_bias = np.argmin(np.abs(bias_new))\n",
    "            bias_new -= bias_new[nearest_zero_bias]\n",
    "            print('bias_new<-- segment')\n",
    "            # 아래쪽 기능을 활용학위해서 nanonisV5 이상인 경우 \n",
    "            # 위쪽 Segment 를 적용했으면 아래쪽  segment 는작동안함 \n",
    "            Segment_V5 = True \n",
    "\n",
    "\n",
    "    else:\n",
    "        bias = Gr.signals['sweep_signal']\n",
    "        Segment_V5 =  False \n",
    "    \n",
    "    Segment = Gr.header['Bias>Bias (V)']\n",
    "    # bias unit : '(V)' \n",
    "\n",
    "    if (type(Segment) == str ) & (Segment_V5 == False): # single segment case\n",
    "        print ('No Segments\\n'+ 'Grid data acquired at bias = '+  str(float(Segment)) + 'V')    \n",
    "    ## No Segments # +  bias setting \n",
    "\n",
    "    ########################\n",
    "    # bias interpolation to have a \"zero\" bias \n",
    "    # interpolate bias_mV that include \"zero\" bias \n",
    "    # in 3D data : center x,y bias interpolation \n",
    "    # e.g  256--> including end points + zero  = 256+1 ( the center is \"0\")\n",
    "        if len(bias)%2==0:\n",
    "            bias_new = np.linspace(bias[0],bias[-1],num=(len(bias)+1)) \n",
    "            # if bias length is even_number \n",
    "            # including \"0\", total size is \"len+1\" \n",
    "        else:# if bias length is odd_number \n",
    "            bias_new = np.linspace(bias[0],bias[-1],num=(len(bias))) \n",
    "            # bias_new make a odd number of length\n",
    "            # make only one value is closest to the zero. \n",
    "            \n",
    "        nearest_zero_bias = np.where(abs(bias_new) == np.amin(abs(bias_new))) \n",
    "        # find the index of closest to \"0\" bias \n",
    "        bias_new = bias_new - bias_new[nearest_zero_bias] \n",
    "        # assign closest zero vavlue as a zero. \n",
    "        #bias_new[np.where(bias_new == np.amin(abs(bias_new)))]=0\n",
    "\n",
    "    ##############################################\n",
    "    #'Segment Start (V), Segment End (V), Settling (s), Integration (s), Steps (xn)'\n",
    "    elif (len(Segment) == 3) & (Segment_V5 == False):\n",
    "        print('Number of Segments =' + str(len(Segment))) \n",
    "        Segments = np.array([[ float(Segments) \n",
    "                              for Segments in Seg.split(',') ] \n",
    "                             for Seg in Segment], dtype = np.float64)\n",
    "        # in the Segment, split strings sith \",\" \n",
    "        #  make a array after change it as float. \n",
    "        # check Nanonispy version\n",
    "        # bias value could be not correct. \n",
    "        \n",
    "        Seg1 = np.linspace(Segments[0,0],Segments[0,1],int(Segments[0,-1]))\n",
    "        Seg2 = np.linspace(Segments[1,0],Segments[1,1],int(Segments[1,-1]))\n",
    "        Seg3 = np.linspace(Segments[2,0],Segments[2,1],int(Segments[2,-1]))\n",
    "        # except boundary end points,  combine segments ([1:]), Seg1, Seg2[1:], Seg3[1:] \n",
    "        bias_Seg = np.append(np.append(Seg1,Seg2[1:]),Seg3[1:]) \n",
    "        # Seg1 +  Seg2[1:] +  Se3[1:] \n",
    "        # make a clever & shoter way 'later...'\n",
    "        print ('bias_Seg size = ' + str(len(bias_Seg)))\n",
    "        bias_Nsteps=int(int(Segments[1,-1])/\n",
    "                        (Seg2[-1]-Seg2[0])*(bias_Seg[-1]-bias_Seg[0]))\n",
    "        # New bias Steps uses smallest step as a new stpe size. \n",
    "        bias_Nsteps_size = (Seg2[-1]-Seg2[0])/(Segments[1,-1])\n",
    "        # (Segments[1,0]-Segments[1,1])/int(Segments[1,-1]) # bias step size    \n",
    "        Neg_bias=-1*np.arange(\n",
    "            0,bias_Nsteps_size*bias_Nsteps/2, bias_Nsteps_size)\n",
    "        Pos_bias=np.flip(\n",
    "            np.arange(0,bias_Nsteps_size*bias_Nsteps/2,bias_Nsteps_size))\n",
    "        bias_new = np.flip( np.append(Pos_bias,Neg_bias[1:])) \n",
    "        # after segments, \n",
    "        # bias is called as  bias_new\n",
    "        ##################################\n",
    "        # now make the bias_new as an odd number. \n",
    "        ###################################\n",
    "        if len(bias_new)%2==0:\n",
    "            bias_new = np.linspace(bias_new[0],bias_new[-1],num=(len(bias_new)+1)) \n",
    "        else:\n",
    "            bias_new = np.linspace(bias_new[0],bias_new[-1],num=(len(bias_new))) \n",
    "        # check  bias_new contians \"zero\" \n",
    "        nearest_zero_bias = np.where(abs(bias_new) == np.amin(abs(bias_new))) \n",
    "        # check index of the nearest value to zero \"0\"\n",
    "        bias_new = bias_new - bias_new[nearest_zero_bias] \n",
    "        # adjust bias range for bias_new has \"zero\" \n",
    "        print ('bias_new size = ' + str(len(bias_new)))\n",
    "        # bias \n",
    "    # make a new list for Bias\n",
    "    else:\n",
    "        if Segment_V5 == False : \n",
    "            print (\"Segment error /n grid2xr is only support 3 segment case at this moment /n code a 5 Sements case\")\n",
    "        else: print ('Segment_V5 == True') \n",
    "    #\n",
    "    ######################################################################\n",
    "    # make a new bias length (including Segments) as a odd number, including zero\n",
    "    ######################################################################\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # interpolation using bias_new \n",
    "    # I_fwd, I_bwd, LIX_fwd, LIX_bwd\n",
    "    # => I_fwd_interpolate\n",
    "    #######################################################################\n",
    "    # assign a function using interpolation \n",
    "    # the same as original bias values \n",
    "    # make empty np array  & interpolate using scipy\n",
    "    # xy dim is not changed here, \n",
    "    # only 3rd axis changed as new bias \n",
    "    ###########################\n",
    "    # Interpolate current and lock-in data to match the new bias values\n",
    "    def sweep_interpolation(np3Ddata, bias, bias_new):\n",
    "        np3Ddata_interpolate = np.empty(\n",
    "                    (np3Ddata.shape[0],np3Ddata.shape[1],bias_new.shape[0])) \n",
    "\n",
    "        for x_i,np3Ddata_xi in enumerate(np3Ddata):\n",
    "            for y_j,np3Ddata_xi_yj in enumerate(np3Ddata_xi):\n",
    "                #print (np3Ddata_xi_yj.shape)\n",
    "                Interpolation1D_i_f = sp.interpolate.interp1d(\n",
    "                    bias,\n",
    "                    np3Ddata_xi_yj,\n",
    "                    fill_value = \"extrapolate\",\n",
    "                    kind = 'cubic')\n",
    "                np3Ddata_interpolate[x_i,y_j,:] = Interpolation1D_i_f(bias_new)\n",
    "        return np3Ddata_interpolate\n",
    "        # bias_mV 생성\n",
    "    bias_mV = bias_new * 1000\n",
    "\n",
    "    # 데이터 보간\n",
    "    I_fwd_interpolate = sweep_interpolation(I_fwd, bias_segment if \n",
    "                                            'Segment Start (V), Segment End (V), Settling (s), Integration (s), Steps (xn), Lockin, Init. Settling (s)' \n",
    "                                            in Gr.header.keys() else bias, bias_new)\n",
    "    I_bwd_interpolate = sweep_interpolation(I_bwd, bias_segment if \n",
    "                                            'Segment Start (V), Segment End (V), Settling (s), Integration (s), Steps (xn), Lockin, Init. Settling (s)' \n",
    "                                            in Gr.header.keys() else bias, bias_new)\n",
    "    LIX_fwd_interpolate = sweep_interpolation(LIX_fwd, bias_segment if \n",
    "                                              'Segment Start (V), Segment End (V), Settling (s), Integration (s), Steps (xn), Lockin, Init. Settling (s)'\n",
    "                                              in Gr.header.keys() else bias, bias_new)\n",
    "    LIX_bwd_interpolate = sweep_interpolation(LIX_bwd, bias_segment if  \n",
    "                                              'Segment Start (V), Segment End (V), Settling (s), Integration (s), Steps (xn), Lockin, Init. Settling (s)'\n",
    "                                              in Gr.header.keys() else bias, bias_new)\n",
    "    '''\n",
    "    I_fwd_interpolate = sweep_interpolation (I_fwd, bias, bias_new)\n",
    "    I_bwd_interpolate = sweep_interpolation (I_bwd, bias, bias_new)\n",
    "    LIX_fwd_interpolate = sweep_interpolation (LIX_fwd, bias, bias_new)\n",
    "    LIX_bwd_interpolate = sweep_interpolation (LIX_bwd, bias, bias_new)\n",
    "    '''\n",
    "    ####################################################\n",
    "    # to prevent error for bias direction \n",
    "    # \n",
    "    ##\n",
    "    #  assign the bias direction \n",
    "    ## up or down ==> up anyway. \n",
    "    ###################################################\n",
    "    if Segment_V5 == True: \n",
    "        print ('check_Segment_V5')\n",
    "        if bias_segment_direction_change == False: \n",
    "            \n",
    "            # if starting point is larger than end point. \n",
    "            # start from pos & end to neg\n",
    "            # no changes. \n",
    "            print ('start from POS bias')\n",
    "            I_fwd = I_fwd_interpolate\n",
    "            I_bwd = I_bwd_interpolate\n",
    "            LIX_fwd = LIX_fwd_interpolate\n",
    "            LIX_bwd = LIX_bwd_interpolate\n",
    "            bias_mV = bias_new*1000\n",
    "        else:  # if end point is larger than start point. \n",
    "            # start from neg & end to pos\n",
    "            # change to negative \n",
    "            \n",
    "            print ('bias_new[0]>bias_new[-1]: False') \n",
    "            print ('start from NEG bias')\n",
    "            \n",
    "            I_fwd = I_fwd_interpolate\n",
    "            I_bwd = I_bwd_interpolate\n",
    "            LIX_fwd = LIX_fwd_interpolate\n",
    "            LIX_bwd = LIX_bwd_interpolate\n",
    "            ## Neg 에서 시작하는 grid 의 경우를 맞춰기위해서 flip없앰. \n",
    "            #I_fwd = np.flip(I_fwd_interpolate,2)\n",
    "            #I_bwd = np.flip(I_bwd_interpolate,2)\n",
    "            #LIX_fwd = np.flip(LIX_fwd_interpolate,2)\n",
    "            #LIX_bwd = np.flip(LIX_bwd_interpolate,2)\n",
    "            #bias_new_flip = np.flip(bias_new)\n",
    "            bias_mV = bias_new*1000\n",
    "            print ('After Flip => now all start from POS bias')\n",
    "        ####################################################\n",
    "\n",
    "    else:      \n",
    "        if bias[0]>bias[-1]: \n",
    "            # if starting point is larger than end point. \n",
    "            # start from pos & end to neg\n",
    "            # no changes. \n",
    "            print ('start from POS bias')\n",
    "            I_fwd = I_fwd_interpolate\n",
    "            I_bwd = I_bwd_interpolate\n",
    "            LIX_fwd = LIX_fwd_interpolate\n",
    "            LIX_bwd = LIX_bwd_interpolate\n",
    "            bias_mV = bias_new*1000\n",
    "        else:  # if end point is larger than start point. \n",
    "            # start from neg & end to pos\n",
    "            # change to negative \n",
    "            print ('start from NEG bias')\n",
    "            I_fwd = np.flip(I_fwd_interpolate,2)\n",
    "            I_bwd = np.flip(I_bwd_interpolate,2)\n",
    "            LIX_fwd = np.flip(LIX_fwd_interpolate,2)\n",
    "            LIX_bwd = np.flip(LIX_bwd_interpolate,2)\n",
    "            bias_new_flip = np.flip(bias_new)\n",
    "            bias_mV = bias_new_flip*1000\n",
    "            print ('Flip => start from POS bias')\n",
    "            ####################################################\n",
    "    \n",
    "    ###################################################\n",
    "    # convert data XR DataSet\n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "    # col = x \n",
    "    # row = y\n",
    "    # I_fwd grid data ==> [Y, X, bias]\n",
    "    print(I_fwd.shape)\n",
    "    grid_xr = xr.Dataset(\n",
    "        {\n",
    "            \"I_fwd\" : ([\"Y\",\"X\",\"bias_mV\"], I_fwd),\n",
    "            \"I_bwd\" : ([\"Y\",\"X\",\"bias_mV\"], I_bwd),\n",
    "            \"LIX_fwd\" : ([\"Y\",\"X\",\"bias_mV\"], LIX_fwd),\n",
    "            \"LIX_bwd\" : ([\"Y\",\"X\",\"bias_mV\"], LIX_bwd),\n",
    "            \"topography\" : ([\"Y\",\"X\"], topography)\n",
    "        },\n",
    "        coords = {\n",
    "            \"X\": ([\"X\"], x),\n",
    "            \"Y\": ([\"Y\"], y),\n",
    "            \"bias_mV\": ([\"bias_mV\"], bias_mV)\n",
    "        }\n",
    "    )\n",
    "    grid_xr.attrs[\"title\"] = title\n",
    "    #grid_xr.attrs['image_size'] = \n",
    "    #grid_xr.attrs['samlpe'] = \n",
    "    \n",
    "    grid_xr.attrs['image_size']= [size_x,size_y]\n",
    "    grid_xr.attrs['X_spacing']= step_dx\n",
    "    grid_xr.attrs['Y_spacing']= step_dy    \n",
    "    #grid_xr.attrs['freq_X_spacing']= 1/step_dx\n",
    "    #grid_xr.attrs['freq_Y_spacing']= 1/step_dy\n",
    "    # use the complex128 = True for xrft, \n",
    "    # then xrdata_fft.freq_X.spacing \n",
    "    # use the attrs in axis info \n",
    "    # in case of real X Y ( center & size of XY)\n",
    "    if center_offset == True:\n",
    "        # move the scan center postion in real scanner field of view\n",
    "        grid_xr.assign_coords( X = (grid_xr.X + cntr_x -  size_x/2))\n",
    "        grid_xr.assign_coords( Y = (grid_xr.Y + cntr_y -  size_y/2))\n",
    "    else :\n",
    "        pass\n",
    "        # (0,0) is the origin of image \n",
    "    \n",
    "\n",
    "    ############################\n",
    "    # check the XY ratio \n",
    "    ############################\n",
    "    #    if  size_x == size_y : \n",
    "    if  dim_px == dim_py : \n",
    "\n",
    "        pass\n",
    "    else : \n",
    "        print ('dim_px != dim_py')\n",
    "    # if xy size is not same, report it! \n",
    "\n",
    "    if step_dx != step_dy :\n",
    "        xystep_ratio = step_dy/step_dx # check the XY pixel_ratio\n",
    "        X_interp = np.linspace(grid_xr.X[0], grid_xr.X[-1], grid_xr.X.shape[0]*1)\n",
    "        step_dx = step_dx # step_dx check \n",
    "\n",
    "        Y_interp = np.linspace(grid_xr.Y[0], grid_xr.Y[-1], int(grid_xr.Y.shape[0]*xystep_ratio)) \n",
    "        step_dy = step_dy/ xystep_ratio # step_dy check \n",
    "\n",
    "        # interpolation ratio should be int\n",
    "        grid_xr= grid_xr.interp(X = X_interp, Y = Y_interp, method=\"linear\")\n",
    "        print('step_dx/step_dy = ', xystep_ratio)\n",
    "        print ('grid_xr ==> reshaped')\n",
    "    else: \n",
    "        grid_xr =grid_xr\n",
    "        print('step_dx == step_dy')\n",
    "    #print('z_LIX_fNb_xr', 'step_dx, step_dy = ',  z_LIX_fNb_xr.dims)\n",
    "    print('grid_xr', 'step_dx, step_dy = ', \n",
    "          re.findall(r'\\{([^}]+)', str(grid_xr.dims)))\n",
    "    # regex practice\n",
    "    \n",
    "    #################################\n",
    "    # assign attributes \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    if 'Wtip' in title:\n",
    "        grid_xr.attrs['tip'] = 'W'\n",
    "    elif 'PtIr' in title:\n",
    "        grid_xr.attrs['tip'] = 'PtIr'\n",
    "    elif '_Ni' in title:\n",
    "        grid_xr.attrs['tip'] = 'Ni'\n",
    "    elif 'Co_coated' in title:\n",
    "        grid_xr.attrs['tip'] = 'Co_coated'\n",
    "    elif 'AFM' in title:\n",
    "        grid_xr.attrs['tip'] = 'AFM'\n",
    "    else: \n",
    "        grid_xr.attrs['tip'] = 'To Be Announced'\n",
    "        print('tip material will be announced later')\n",
    "    \n",
    "    if 'NbSe2' in title:\n",
    "        grid_xr.attrs['sample'] = 'NbSe2'\n",
    "    elif 'Cu(111)' in title:\n",
    "        grid_xr.attrs['sample'] = 'Cu(111)'\n",
    "    elif 'Au(111)' in title:\n",
    "        grid_xr.attrs['sample'] = 'Au(111)'\n",
    "    elif 'MoS2' in title:\n",
    "        grid_xr.attrs['sample'] = 'MoS2'\n",
    "    elif 'FeTe0.55Se0.45' in title:\n",
    "        grid_xr.attrs['sample'] = 'FeTe0.55Se0.45'\n",
    "    else: \n",
    "        grid_xr.attrs['sample'] = 'To Be Announced'\n",
    "        print('sample type will be announced later')\n",
    "        \n",
    "    if '40mK' in title:\n",
    "        grid_xr.attrs['temperature'] = '40mK'\n",
    "    elif 'LHe' in title:\n",
    "        grid_xr.attrs['temperature'] = 'LHe'\n",
    "    elif 'LN2T' in title:\n",
    "        grid_xr.attrs['temperature'] = 'LN2T'\n",
    "    elif 'RT' in title:\n",
    "        grid_xr.attrs['temperature'] = 'RT'\n",
    "    else: \n",
    "        grid_xr.attrs['temperature'] = 'To Be Announced'\n",
    "        print('temperature will be announced later')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return grid_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e584f",
   "metadata": {},
   "source": [
    "## `img2xr()` (SXM → `xarray.Dataset`)\n",
    "\n",
    "This is the same `img2xr_updated` logic you provided (robust multipass detection + NetCDF-safe attrs),\n",
    "kept as an I/O-only function.\n",
    "\n",
    "Stage-0 should ensure dependencies are available. If `nanonispy` is missing, this function raises a clear error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0335eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2xr(loading_sxm_file: str, center_offset: bool = False) -> xr.Dataset:\n",
    "    \"\"\"Load a Nanonis .sxm file and convert it to an xarray.Dataset.\n",
    "\n",
    "    Multipass detection order:\n",
    "      (1) exact header key 'multipass-config'\n",
    "      (2) any header key containing 'multipass' (case-insensitive)\n",
    "      (3) any signal key containing 'P<number>'\n",
    "\n",
    "    Produces NetCDF-safe attributes (dicts/lists-with-dicts serialized as JSON).\n",
    "\n",
    "    IMPORTANT:\n",
    "    - In single-pass mode, LIX is detected ONLY by names containing LI and X (case-insensitive).\n",
    "      CURRENT is not treated as a substitute for LIX.\n",
    "    - CURRENT, if present, is saved separately as CURR_fwd / CURR_bwd.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import nanonispy as nap\n",
    "    except ModuleNotFoundError as e:\n",
    "        raise ModuleNotFoundError(\n",
    "            \"nanonispy is required for reading .sxm files. \"\n",
    "            \"Please install dependencies (run env_check / Stage-0) and restart the kernel.\"\n",
    "        ) from e\n",
    "\n",
    "    # ----------------------------- NetCDF-safe attribute sanitizer -----------------------------\n",
    "    def _sanitize_attr_value(v):\n",
    "        if isinstance(v, (bool, np.bool_)):\n",
    "            return int(v)\n",
    "        if isinstance(v, (np.integer,)):\n",
    "            return int(v)\n",
    "        if isinstance(v, (np.floating,)):\n",
    "            return float(v)\n",
    "        if isinstance(v, dict):\n",
    "            try:\n",
    "                return json.dumps(v, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                return str(v)\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            if any(isinstance(x, dict) for x in v):\n",
    "                try:\n",
    "                    return json.dumps(v, ensure_ascii=False)\n",
    "                except Exception:\n",
    "                    return str(v)\n",
    "            out = []\n",
    "            for x in v:\n",
    "                if isinstance(x, (bool, np.bool_)):\n",
    "                    out.append(int(x))\n",
    "                elif isinstance(x, (np.integer,)):\n",
    "                    out.append(int(x))\n",
    "                elif isinstance(x, (np.floating,)):\n",
    "                    out.append(float(x))\n",
    "                elif isinstance(x, np.ndarray):\n",
    "                    if x.dtype.kind in (\"i\", \"u\", \"f\"):\n",
    "                        out.extend([float(xx) for xx in x.ravel().tolist()])\n",
    "                    else:\n",
    "                        out.append(str(x.tolist()))\n",
    "                elif isinstance(x, bytes):\n",
    "                    out.append(x.decode(\"utf-8\", errors=\"ignore\"))\n",
    "                elif isinstance(x, (str, int, float)):\n",
    "                    out.append(x)\n",
    "                else:\n",
    "                    out.append(str(x))\n",
    "            return out\n",
    "        if isinstance(v, np.ndarray):\n",
    "            if v.dtype.kind in (\"i\", \"u\", \"f\"):\n",
    "                return v.astype(float).tolist()\n",
    "            return str(v.tolist())\n",
    "        if v is None:\n",
    "            return \"\"\n",
    "        if isinstance(v, bytes):\n",
    "            return v.decode(\"utf-8\", errors=\"ignore\")\n",
    "        if isinstance(v, str):\n",
    "            return v\n",
    "        return str(v)\n",
    "\n",
    "    def _sanitize_dataset_attrs(ds: xr.Dataset) -> xr.Dataset:\n",
    "        ds = ds.copy()\n",
    "        ds.attrs = {k: _sanitize_attr_value(v) for k, v in ds.attrs.items()}\n",
    "        for name, var in ds.variables.items():\n",
    "            if var.attrs:\n",
    "                var.attrs = {k: _sanitize_attr_value(v) for k, v in var.attrs.items()}\n",
    "        return ds\n",
    "\n",
    "    # ----------------------------- helpers -----------------------------\n",
    "    _NUM_PAT = re.compile(r\"[-+]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?\")\n",
    "\n",
    "    def parse_signed_float(x, *, assume_unit=\"V\", allow_nan=True):\n",
    "        if x is None:\n",
    "            return np.nan if allow_nan else 0.0\n",
    "        if isinstance(x, (int, float, np.number)):\n",
    "            return float(x)\n",
    "        s = str(x).strip()\n",
    "        if s == \"\" or s.upper() in {\"N/A\", \"NA\", \"NONE\"}:\n",
    "            return np.nan if allow_nan else 0.0\n",
    "        s = (s.replace(\"\\u2212\", \"-\")\n",
    "               .replace(\"\\u2013\", \"-\")\n",
    "               .replace(\"\\u2014\", \"-\")\n",
    "               .replace(\"\\u2213\", \"+/-\"))\n",
    "        has_mV = \"mv\" in s.lower()\n",
    "        has_V = \" v\" in s.lower() or s.lower().endswith(\"v\")\n",
    "        s_clean = (s.replace(\",\", \" \")\n",
    "                   .replace(\"mV\", \" \").replace(\"MV\", \" \")\n",
    "                   .replace(\"v\", \" \").replace(\"V\", \" \")\n",
    "                   .strip())\n",
    "        m = _NUM_PAT.search(s_clean)\n",
    "        if not m:\n",
    "            try:\n",
    "                return float(s)\n",
    "            except Exception:\n",
    "                return np.nan if allow_nan else 0.0\n",
    "        val = float(m.group(0))\n",
    "        if has_mV and not has_V:\n",
    "            return val / 1000.0\n",
    "        if (not has_mV) and (not has_V):\n",
    "            return val if assume_unit.upper() == \"V\" else (val / 1000.0 if assume_unit.upper() == \"MV\" else val)\n",
    "        return val\n",
    "\n",
    "    def as_float_scalar(a):\n",
    "        try:\n",
    "            return float(np.asarray(a).item() if np.ndim(a) == 0 else np.asarray(a)[()])\n",
    "        except Exception:\n",
    "            return float(a)\n",
    "\n",
    "    def flip_xy(arr, flip_x=False, flip_y=False):\n",
    "        if flip_x:\n",
    "            arr = arr[:, ::-1]\n",
    "        if flip_y:\n",
    "            arr = arr[::-1, :]\n",
    "        return arr\n",
    "\n",
    "    def fillna_mean(arr):\n",
    "        a = np.array(arr, dtype=float)\n",
    "        if np.isnan(a).any():\n",
    "            m = np.nanmean(a)\n",
    "            if np.isnan(m):\n",
    "                m = 0.0\n",
    "            a = np.nan_to_num(a, nan=m)\n",
    "        return a\n",
    "\n",
    "    def classify_channel_name(sig_key):\n",
    "        sk = str(sig_key).upper()\n",
    "        m = re.search(r\"P\\s*(\\d+)\", sk)\n",
    "        pidx = int(m.group(1)) if m else None\n",
    "        if \"Z\" in sk and \"LI\" not in sk:\n",
    "            kind = \"Z\"\n",
    "        elif \"LI\" in sk and \"X\" in sk:\n",
    "            kind = \"LI_X\"\n",
    "        elif \"LI\" in sk and \"Y\" in sk:\n",
    "            kind = \"LI_Y\"\n",
    "        elif \"CURRENT\" in sk:\n",
    "            kind = \"Current\"\n",
    "        else:\n",
    "            kind = \"Other\"\n",
    "        return kind, pidx\n",
    "\n",
    "    # ----------------------------- open file + tolerant header access -----------------------------\n",
    "    NF = nap.read.NanonisFile(loading_sxm_file)\n",
    "    Scan = nap.read.Scan(NF.fname)\n",
    "\n",
    "    def get_case_insensitive_key(d, candidates):\n",
    "        if not isinstance(d, dict):\n",
    "            return None\n",
    "        low = {str(k).lower(): k for k in d.keys()}\n",
    "        for c in candidates:\n",
    "            if c.lower() in low:\n",
    "                return low[c.lower()]\n",
    "        return None\n",
    "\n",
    "    k_bias = get_case_insensitive_key(Scan.header, [\"bias>bias (v)\", \"bias>bias (V)\", \"bias\"])\n",
    "    k_setpt = get_case_insensitive_key(Scan.header, [\"z-controller>setpoint\", \"z-controller>setpoint (A)\", \"setpoint\"])\n",
    "\n",
    "    V_b = parse_signed_float(Scan.header[k_bias], assume_unit=\"V\") if k_bias else np.nan\n",
    "    I_t = parse_signed_float(Scan.header[k_setpt], assume_unit=\"A\") if k_setpt else np.nan\n",
    "    if np.isnan(V_b):\n",
    "        V_b = 0.0\n",
    "    if np.isnan(I_t):\n",
    "        I_t = 0.0\n",
    "\n",
    "    size_x, size_y = Scan.header[\"scan_range\"]\n",
    "    cntr_x, cntr_y = Scan.header[\"scan_offset\"]\n",
    "    dim_px, dim_py = Scan.header[\"scan_pixels\"]\n",
    "    step_dx, step_dy = size_x / dim_px, size_y / dim_py\n",
    "    Rot_Rad = math.radians(float(Scan.header[\"scan_angle\"]))\n",
    "    scan_dir = Scan.header.get(\"scan_dir\", \"up\")\n",
    "    basename = getattr(Scan, \"basename\", NF.fname)\n",
    "\n",
    "    # ----------------------------- multipass detection -----------------------------\n",
    "    if \"multipass-config\" in Scan.header.keys():\n",
    "        mp_cfg = Scan.header.get(\"multipass-config\", {})\n",
    "        is_multipass = True\n",
    "    else:\n",
    "        header_keys_lower = {str(k).lower(): k for k in Scan.header.keys()}\n",
    "        mp_header_key = None\n",
    "        for lk, orig in header_keys_lower.items():\n",
    "            if \"multipass\" in lk:\n",
    "                mp_header_key = orig\n",
    "                break\n",
    "        has_mp_cfg = mp_header_key is not None\n",
    "        mp_cfg = Scan.header.get(mp_header_key, {}) if has_mp_cfg else {}\n",
    "        if not isinstance(mp_cfg, dict):\n",
    "            mp_cfg = {}\n",
    "        has_pnum = any(re.search(r\"P\\s*\\d+\", str(k), flags=re.IGNORECASE) for k in Scan.signals.keys())\n",
    "        is_multipass = bool(has_mp_cfg or has_pnum)\n",
    "\n",
    "    # ----------------------------- coords -----------------------------\n",
    "    X_idx = np.arange(dim_px)\n",
    "    Y_idx = np.arange(dim_py)\n",
    "    X_coords = (X_idx + 0.5) * step_dx\n",
    "    Y_coords = (Y_idx + 0.5) * step_dy\n",
    "    ds = xr.Dataset(coords=dict(X=(\"X\", X_coords), Y=(\"Y\", Y_coords)))\n",
    "\n",
    "    # ----------------------------- multipass path -----------------------------\n",
    "    if is_multipass:\n",
    "        bias_map = {}\n",
    "        values = mp_cfg.get(\"Bias_override_value\", [])\n",
    "        if isinstance(values, (str, int, float)):\n",
    "            values = [values]\n",
    "        try:\n",
    "            vals = [parse_signed_float(v, assume_unit=\"V\") for v in values]\n",
    "            vals = [float(v) for v in vals if not np.isnan(v)]\n",
    "        except Exception:\n",
    "            vals = []\n",
    "        n_passes = len(vals) // 2 if len(vals) >= 2 else 0\n",
    "        if n_passes > 0:\n",
    "            for k in range(n_passes):\n",
    "                bias_map[(k + 1, \"forward\")] = float(vals[2 * k + 0])\n",
    "                bias_map[(k + 1, \"backward\")] = float(vals[2 * k + 1])\n",
    "\n",
    "        channels_by_pass = {}\n",
    "        for key in Scan.signals.keys():\n",
    "            kind, pidx = classify_channel_name(key)\n",
    "            if pidx is None:\n",
    "                continue\n",
    "            channels_by_pass.setdefault(pidx, {})\n",
    "            if kind not in channels_by_pass[pidx]:\n",
    "                channels_by_pass[pidx][kind] = key\n",
    "\n",
    "        flip_y = str(scan_dir).lower() == \"down\"\n",
    "\n",
    "        def _name_for(k):\n",
    "            return \"Z\" if k == \"Z\" else (\"LIX\" if k == \"LI_X\" else (\"LIY\" if k == \"LI_Y\" else \"CURR\"))\n",
    "\n",
    "        for pidx in sorted(channels_by_pass.keys()):\n",
    "            pass_ch = channels_by_pass[pidx]\n",
    "            kinds = []\n",
    "            if \"Z\" in pass_ch:\n",
    "                kinds.append(\"Z\")\n",
    "            if \"LI_X\" in pass_ch:\n",
    "                kinds.append(\"LI_X\")\n",
    "            elif \"Current\" in pass_ch:\n",
    "                kinds.append(\"Current\")\n",
    "            if \"LI_Y\" in pass_ch:\n",
    "                kinds.append(\"LI_Y\")\n",
    "\n",
    "            for kind in kinds:\n",
    "                src_key = pass_ch[kind]\n",
    "                sig = Scan.signals[src_key]\n",
    "                fwd = np.array(sig.get(\"forward\", None)) if \"forward\" in sig else None\n",
    "                bwd = np.array(sig.get(\"backward\", None)) if \"backward\" in sig else None\n",
    "\n",
    "                if fwd is not None:\n",
    "                    fwd = fillna_mean(fwd)\n",
    "                    fwd = flip_xy(fwd, flip_x=False, flip_y=flip_y)\n",
    "                if bwd is not None:\n",
    "                    bwd = fillna_mean(bwd)\n",
    "                    bwd = flip_xy(bwd, flip_x=True, flip_y=flip_y)\n",
    "\n",
    "                bias_fwd_V = float(bias_map.get((pidx, \"forward\"), V_b))\n",
    "                bias_bwd_V = float(bias_map.get((pidx, \"backward\"), V_b))\n",
    "\n",
    "                if fwd is not None:\n",
    "                    var_name = f\"{_name_for(kind)}_P{pidx}_fwd\"\n",
    "                    ds[var_name] = xr.DataArray(fwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "                    ds[var_name].attrs.update(\n",
    "                        dict(\n",
    "                            kind=(\"Z\" if kind == \"Z\" else (\"LI_X\" if kind == \"LI_X\" else (\"LI_Y\" if kind == \"LI_Y\" else \"Current\"))),\n",
    "                            pass_index=int(pidx),\n",
    "                            direction=\"forward\",\n",
    "                            bias_V=bias_fwd_V,\n",
    "                            bias_mV=1000.0 * bias_fwd_V,\n",
    "                            setpoint_A=I_t,\n",
    "                            bias_set_V=V_b,\n",
    "                            scan_angle_deg=float(math.degrees(Rot_Rad)),\n",
    "                            source_channel_name=str(src_key),\n",
    "                            flip_applied_Y=bool(flip_y),\n",
    "                            nan_filled=\"mean_of_array\",\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                if bwd is not None:\n",
    "                    var_name = f\"{_name_for(kind)}_P{pidx}_bwd\"\n",
    "                    ds[var_name] = xr.DataArray(bwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "                    ds[var_name].attrs.update(\n",
    "                        dict(\n",
    "                            kind=(\"Z\" if kind == \"Z\" else (\"LI_X\" if kind == \"LI_X\" else (\"LI_Y\" if kind == \"LI_Y\" else \"Current\"))),\n",
    "                            pass_index=int(pidx),\n",
    "                            direction=\"backward\",\n",
    "                            bias_V=bias_bwd_V,\n",
    "                            bias_mV=1000.0 * bias_bwd_V,\n",
    "                            setpoint_A=I_t,\n",
    "                            bias_set_V=V_b,\n",
    "                            scan_angle_deg=float(math.degrees(Rot_Rad)),\n",
    "                            source_channel_name=str(src_key),\n",
    "                            flip_applied_Y=bool(flip_y),\n",
    "                            nan_filled=\"mean_of_array\",\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        if not np.isclose(step_dx, step_dy):\n",
    "            ny, nx = ds.sizes[\"Y\"], ds.sizes[\"X\"]\n",
    "            x0, x1 = as_float_scalar(ds[\"X\"].values[0]), as_float_scalar(ds[\"X\"].values[-1])\n",
    "            y0, y1 = as_float_scalar(ds[\"Y\"].values[0]), as_float_scalar(ds[\"Y\"].values[-1])\n",
    "            ratio = step_dy / step_dx\n",
    "            ny_new = max(int(round(ny * ratio)), 1)\n",
    "            X_new = np.linspace(float(x0), float(x1), int(nx))\n",
    "            Y_new = np.linspace(float(y0), float(y1), int(ny_new))\n",
    "            ds = ds.interp(X=X_new, Y=Y_new, method=\"linear\")\n",
    "            eff_dx = (float(X_new[-1]) - float(X_new[0])) / max(len(X_new) - 1, 1)\n",
    "            eff_dy = (float(Y_new[-1]) - float(Y_new[0])) / max(len(Y_new) - 1, 1)\n",
    "        else:\n",
    "            eff_dx, eff_dy = step_dx, step_dy\n",
    "\n",
    "        if not center_offset:\n",
    "            ds = ds.assign_coords(\n",
    "                X=(ds[\"X\"] + (cntr_x - size_x / 2.0)),\n",
    "                Y=(ds[\"Y\"] + (cntr_y - size_y / 2.0)),\n",
    "            )\n",
    "\n",
    "        ds.attrs.update(\n",
    "            dict(\n",
    "                multipass=True,\n",
    "                n_passes=int(len(channels_by_pass.keys())) if channels_by_pass else 1,\n",
    "                image_size=[float(size_x), float(size_y)],\n",
    "                X_spacing=float(eff_dx),\n",
    "                Y_spacing=float(eff_dy),\n",
    "                scan_angle_deg=float(math.degrees(Rot_Rad)),\n",
    "                scan_dir=str(scan_dir),\n",
    "                data_vars_list=list(ds.data_vars.keys()),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ds = _sanitize_dataset_attrs(ds)\n",
    "        return ds\n",
    "\n",
    "    # ----------------------------- single-pass path -----------------------------\n",
    "    def _prep(a, flipx=False, flipy=False):\n",
    "        if a is None:\n",
    "            return None\n",
    "        a = fillna_mean(a)\n",
    "        return flip_xy(a, flip_x=flipx, flip_y=flipy)\n",
    "\n",
    "    z_fwd = np.array(Scan.signals[\"Z\"][\"forward\"]) if \"Z\" in Scan.signals else None\n",
    "    z_bwd = np.array(Scan.signals[\"Z\"][\"backward\"])[:, ::-1] if \"Z\" in Scan.signals else None\n",
    "\n",
    "    # STRICT LIX (no CURRENT fallback)\n",
    "    lix_key = None\n",
    "    for k in Scan.signals.keys():\n",
    "        s = str(k).upper()\n",
    "        if (\"LI\" in s) and (\"X\" in s):\n",
    "            lix_key = k\n",
    "            break\n",
    "    lix_fwd = np.array(Scan.signals[lix_key][\"forward\"]) if lix_key else None\n",
    "    lix_bwd = np.array(Scan.signals[lix_key][\"backward\"])[:, ::-1] if lix_key else None\n",
    "\n",
    "    liy_key = None\n",
    "    for k in Scan.signals.keys():\n",
    "        s = str(k).upper()\n",
    "        if (\"LI\" in s) and (\"Y\" in s):\n",
    "            liy_key = k\n",
    "            break\n",
    "    liy_fwd = np.array(Scan.signals[liy_key][\"forward\"]) if liy_key else None\n",
    "    liy_bwd = np.array(Scan.signals[liy_key][\"backward\"])[:, ::-1] if liy_key else None\n",
    "\n",
    "    curr_key = None\n",
    "    for k in Scan.signals.keys():\n",
    "        if \"CURRENT\" in str(k).upper():\n",
    "            curr_key = k\n",
    "            break\n",
    "    curr_fwd = np.array(Scan.signals[curr_key][\"forward\"]) if curr_key else None\n",
    "    curr_bwd = np.array(Scan.signals[curr_key][\"backward\"])[:, ::-1] if curr_key else None\n",
    "\n",
    "    flip_y = str(Scan.header.get(\"scan_dir\", \"up\")).lower() == \"down\"\n",
    "    z_fwd = _prep(z_fwd, flipx=False, flipy=flip_y)\n",
    "    z_bwd = _prep(z_bwd, flipx=False, flipy=flip_y)\n",
    "    lix_fwd = _prep(lix_fwd, flipx=False, flipy=flip_y)\n",
    "    lix_bwd = _prep(lix_bwd, flipx=False, flipy=flip_y)\n",
    "    liy_fwd = _prep(liy_fwd, flipx=False, flipy=flip_y)\n",
    "    liy_bwd = _prep(liy_bwd, flipx=False, flipy=flip_y)\n",
    "    curr_fwd = _prep(curr_fwd, flipx=False, flipy=flip_y)\n",
    "    curr_bwd = _prep(curr_bwd, flipx=False, flipy=flip_y)\n",
    "\n",
    "    if z_fwd is not None:\n",
    "        ds[\"Z_fwd\"] = xr.DataArray(z_fwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if z_bwd is not None:\n",
    "        ds[\"Z_bwd\"] = xr.DataArray(z_bwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if lix_fwd is not None:\n",
    "        ds[\"LIX_fwd\"] = xr.DataArray(lix_fwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if lix_bwd is not None:\n",
    "        ds[\"LIX_bwd\"] = xr.DataArray(lix_bwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if liy_fwd is not None:\n",
    "        ds[\"LIY_fwd\"] = xr.DataArray(liy_fwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if liy_bwd is not None:\n",
    "        ds[\"LIY_bwd\"] = xr.DataArray(liy_bwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if curr_fwd is not None:\n",
    "        ds[\"CURR_fwd\"] = xr.DataArray(curr_fwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "    if curr_bwd is not None:\n",
    "        ds[\"CURR_bwd\"] = xr.DataArray(curr_bwd, dims=(\"Y\", \"X\"), coords={\"Y\": ds[\"Y\"], \"X\": ds[\"X\"]})\n",
    "\n",
    "    if not np.isclose(step_dx, step_dy):\n",
    "        ny, nx = ds.sizes[\"Y\"], ds.sizes[\"X\"]\n",
    "        x0, x1 = as_float_scalar(ds[\"X\"].values[0]), as_float_scalar(ds[\"X\"].values[-1])\n",
    "        y0, y1 = as_float_scalar(ds[\"Y\"].values[0]), as_float_scalar(ds[\"Y\"].values[-1])\n",
    "        ratio = step_dy / step_dx\n",
    "        ny_new = max(int(round(ny * ratio)), 1)\n",
    "        X_new = np.linspace(float(x0), float(x1), int(nx))\n",
    "        Y_new = np.linspace(float(y0), float(y1), int(ny_new))\n",
    "        ds = ds.interp(X=X_new, Y=Y_new, method=\"linear\")\n",
    "        eff_dx = (float(X_new[-1]) - float(X_new[0])) / max(len(X_new) - 1, 1)\n",
    "        eff_dy = (float(Y_new[-1]) - float(Y_new[0])) / max(len(Y_new) - 1, 1)\n",
    "    else:\n",
    "        eff_dx, eff_dy = step_dx, step_dy\n",
    "\n",
    "    if not center_offset:\n",
    "        ds = ds.assign_coords(\n",
    "            X=(ds[\"X\"] + (cntr_x - size_x / 2.0)),\n",
    "            Y=(ds[\"Y\"] + (cntr_y - size_y / 2.0)),\n",
    "        )\n",
    "\n",
    "    ds.attrs.update(\n",
    "        dict(\n",
    "            multipass=False,\n",
    "            n_passes=1,\n",
    "            image_size=[float(size_x), float(size_y)],\n",
    "            X_spacing=float(eff_dx),\n",
    "            Y_spacing=float(eff_dy),\n",
    "            scan_angle_deg=float(math.degrees(Rot_Rad)),\n",
    "            scan_dir=str(scan_dir),\n",
    "            data_vars_list=list(ds.data_vars.keys()),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ds = _sanitize_dataset_attrs(ds)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf486950-6c1b-4932-893d-93f1b0661a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
